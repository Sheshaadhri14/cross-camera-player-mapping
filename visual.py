# visualize_mapping.py
# This script loads the JSON outputs from main.py and generates annotated video files.

import cv2
import json
import os
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

# --- 1. CONFIGURATION ---

# Input and Output directories (should match your main.py setup)
INPUT_DIR = "INPUT"
OUTPUT_DIR = "OUTPUT"

# Paths to original video files
BROADCAST_VIDEO_PATH = os.path.join(INPUT_DIR, "broadcast.mp4")
TACTICAM_VIDEO_PATH = os.path.join(INPUT_DIR, "tacticam.mp4")

# Paths to JSON files generated by main.py
FINAL_MAPPING_FILE = os.path.join(OUTPUT_DIR, "final_mapping.json")
BROADCAST_FEATURES_FILE = os.path.join(OUTPUT_DIR, "broadcast_features.json")
TACTICAM_FEATURES_FILE = os.path.join(OUTPUT_DIR, "tacticam_features.json")

# Output video filenames
BROADCAST_OUTPUT_VIDEO = os.path.join(OUTPUT_DIR, "broadcast_annotated.mp4")
TACTICAM_OUTPUT_VIDEO = os.path.join(OUTPUT_DIR, "tacticam_annotated.mp4")
EVAL_REPORT_FILE = os.path.join(OUTPUT_DIR, "mapping_evaluation_report.json")
EVAL_PLOT_FILE = os.path.join(OUTPUT_DIR, "mapping_eval_chart.png")

# --- 2. HELPER FUNCTIONS ---

def load_json_data(filepath):
    if not os.path.exists(filepath):
        print(f"Error: Required file not found: {filepath}")
        return None
    with open(filepath, 'r') as f:
        return json.load(f)

def preprocess_tracks_data(tracks_json_data):
    frame_to_tracks = {}
    center_lookup = {}
    for track_id_str, track_info in tracks_json_data.items():
        track_id = track_id_str
        if 'boxes' in track_info:
            centers = []
            for frame_idx, x1, y1, x2, y2 in track_info['boxes']:
                center = [(x1 + x2) // 2, (y1 + y2) // 2]
                centers.append(center)
                if frame_idx not in frame_to_tracks:
                    frame_to_tracks[frame_idx] = {}
                frame_to_tracks[frame_idx][track_id] = [x1, y1, x2, y2]
            if centers:
                avg = np.mean(centers, axis=0)
                center_lookup[track_id] = avg
    return frame_to_tracks, center_lookup

def draw_annotations_on_frame(frame, current_frame_tracks, mapping, reverse_mapping, is_tacticam_video, eval_stats=None):
    for track_id, bbox in current_frame_tracks.items():
        x1, y1, x2, y2 = bbox
        color = (255, 0, 0)
        text_label = f"ID: {track_id}"

        if is_tacticam_video:
            if track_id in mapping:
                mapped_broadcast_id = mapping[track_id]['broadcast_id'] if isinstance(mapping[track_id], dict) else mapping[track_id]
                confidence = mapping[track_id].get('confidence', 1.0) if isinstance(mapping[track_id], dict) else 1.0
                distance = eval_stats.get(track_id, {}).get("euclidean_distance", 0.0) if eval_stats else 0.0
                color = (0, 255, 0) if distance <= 100 else (0, 0, 255)
                text_label = f"Tac: {track_id} -> Brd: {mapped_broadcast_id} ({confidence:.2f})"
        else:
            if track_id in reverse_mapping:
                mapped_tacticam_id = reverse_mapping[track_id]
                color = (0, 255, 0)
                text_label = f"Brd: {track_id} -> Tac: {mapped_tacticam_id}"

        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        text_pos = (x1, y1 - 10 if y1 - 10 > 10 else y1 + 20)
        cv2.putText(frame, text_label, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)

    return frame

def visualize_video(video_path, features_data, mapping, reverse_mapping, output_filename, is_tacticam_video, eval_stats):
    print(f"Generating annotated video for: {video_path}")
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video {video_path}.")
        return

    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))

    frame_idx = 0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    pbar = tqdm(total=total_frames, desc=f"Annotating {'Tacticam' if is_tacticam_video else 'Broadcast'}")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        current_frame_tracks = features_data.get(frame_idx, {})
        annotated_frame = draw_annotations_on_frame(frame, current_frame_tracks, mapping, reverse_mapping, is_tacticam_video, eval_stats)
        out.write(annotated_frame)
        frame_idx += 1
        pbar.update(1)

    cap.release()
    out.release()
    pbar.close()
    print(f"Annotated video saved to: {output_filename}")

def evaluate_mapping(tacticam_centers, broadcast_centers, final_mapping):
    print("\nEvaluating mapping accuracy...")
    stats = {}
    distances = []
    for tac_id, mapping in final_mapping.items():
        brd_id = mapping['broadcast_id'] if isinstance(mapping, dict) else mapping
        tac_center = tacticam_centers.get(tac_id)
        brd_center = broadcast_centers.get(brd_id)
        if tac_center is not None and brd_center is not None:
            distance = np.linalg.norm(tac_center - brd_center)
            distances.append(distance)
            confidence = mapping.get("confidence", 1.0) if isinstance(mapping, dict) else 1.0
            stats[tac_id] = {
                "mapped_broadcast_id": brd_id,
                "euclidean_distance": float(distance),
                "confidence": float(confidence)
            }
    summary = {
        "mean_distance": float(np.mean(distances)) if distances else None,
        "std_distance": float(np.std(distances)) if distances else None,
        "max_distance": float(np.max(distances)) if distances else None,
        "worst_match": max(stats.items(), key=lambda x: x[1]["euclidean_distance"])[0] if stats else None
    }
    stats["summary"] = summary
    with open(EVAL_REPORT_FILE, 'w') as f:
        json.dump(stats, f, indent=4)
    print(f"Mapping evaluation report saved to: {EVAL_REPORT_FILE}")

    # plot
    plt.figure(figsize=(10, 5))
    ids = list(stats.keys())[:-1]  # exclude summary
    values = [stats[k]["euclidean_distance"] for k in ids]
    plt.bar(ids, values, color='skyblue')
    plt.axhline(y=100, color='r', linestyle='--', label='Threshold')
    plt.xlabel("Tacticam ID")
    plt.ylabel("Euclidean Distance")
    plt.title("Mapping Distance per Player")
    plt.legend()
    plt.savefig(EVAL_PLOT_FILE)
    plt.close()

# --- 3. MAIN EXECUTION ---

def main():
    print("Starting visualization script...")

    final_mapping = load_json_data(FINAL_MAPPING_FILE)
    broadcast_raw_features = load_json_data(BROADCAST_FEATURES_FILE)
    tacticam_raw_features = load_json_data(TACTICAM_FEATURES_FILE)

    if not all([final_mapping, broadcast_raw_features, tacticam_raw_features]):
        print("Exiting: One or more required JSON files could not be loaded.")
        return

    broadcast_features_by_frame, broadcast_centers = preprocess_tracks_data(broadcast_raw_features)
    tacticam_features_by_frame, tacticam_centers = preprocess_tracks_data(tacticam_raw_features)
    reverse_mapping = {v['broadcast_id'] if isinstance(v, dict) else v: k for k, v in final_mapping.items()}

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    eval_stats = evaluate_mapping(tacticam_centers, broadcast_centers, final_mapping)

    visualize_video(
        BROADCAST_VIDEO_PATH,
        broadcast_features_by_frame,
        final_mapping,
        reverse_mapping,
        BROADCAST_OUTPUT_VIDEO,
        is_tacticam_video=False,
        eval_stats=eval_stats
    )

    visualize_video(
        TACTICAM_VIDEO_PATH,
        tacticam_features_by_frame,
        final_mapping,
        reverse_mapping,
        TACTICAM_OUTPUT_VIDEO,
        is_tacticam_video=True,
        eval_stats=eval_stats
    )

    print("\nâœ… Visualization + Evaluation complete! Check the 'OUTPUT' folder for videos and evaluation report.")

if __name__ == "__main__":
    main()
