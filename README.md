Hereâ€™s your complete `README.md` content formatted and ready to copy-paste into your project directory:

---

```markdown
# ğŸŸï¸ Cross-Camera Player Mapping System

This project maps players between a **Tacticam (top-view)** and **Broadcast (side-view)** camera using:
- ğŸ¯ YOLOv8 object detection (`best.pt`)
- ğŸ“ DeepSORT tracking
- ğŸ‘• OSNet-based Re-Identification
- ğŸ”„ Automatic Homography Estimation
- ğŸ“Š Evaluation + Annotated Video Output

---

## ğŸ“ Project Structure

```

.
â”œâ”€â”€ INPUT/                       # Put your videos here
â”‚   â”œâ”€â”€ broadcast.mp4
â”‚   â””â”€â”€ tacticam.mp4
â”œâ”€â”€ OUTPUT/                      # All output files will be saved here
â”‚   â”œâ”€â”€ final\_mapping.json
â”‚   â”œâ”€â”€ broadcast\_annotated.mp4
â”‚   â”œâ”€â”€ tacticam\_annotated.mp4
â”‚   â””â”€â”€ mapping\_evaluation\_report.json
â”œâ”€â”€ best.pt                      # YOLOv8 custom weights
â”œâ”€â”€ main.py                      # Full pipeline execution
â”œâ”€â”€ visualize\_mapping.py         # Visualization + evaluation
â””â”€â”€ requirements.txt             # Python dependencies

````

---

## âœ… Setup Instructions

### 1. ğŸ Create a virtual environment

```bash
python -m venv venv
# Activate it:
source venv/bin/activate      # macOS/Linux
venv\Scripts\activate         # Windows
````

### 2. ğŸ“¦ Install dependencies

```bash
pip install -r requirements.txt
```

### `requirements.txt` should include:

```
ultralytics
torch
torchvision
torchreid
opencv-python
deep_sort_realtime
scipy
numpy
tqdm
gdown
matplotlib
```

> ğŸ” Pretrained OSNet weights auto-downloaded via `torchreid`.

---

## ğŸš€ How to Run

### ğŸ”§ Step 1: Run full pipeline

```bash
python main.py
```

* Performs detection, tracking, homography estimation, and Re-ID
* Saves mapping in `OUTPUT/final_mapping.json`

---

### ğŸ¥ Step 2: Annotate videos & evaluate

```bash
python visualize_mapping.py
```

* Generates:

  * `broadcast_annotated.mp4`
  * `tacticam_annotated.mp4`
  * `mapping_evaluation_report.json`
  * `mapping_eval_chart.png` (optional)

> ğŸŸ© Good matches are green, mismatches are red, with match confidence overlayed.

---

## ğŸ“Š Evaluation Example

```json
{
  "2": {
    "mapped_broadcast_id": "2",
    "euclidean_distance": 13.85,
    "confidence": 0.88
  },
  ...
  "_summary": {
    "mean_distance": 41.3,
    "std_distance": 20.5,
    "max_distance": 93.7,
    "worst_match": "12"
  }
}
```

---

## âš ï¸ Notes

* Input videos must be named:

  * `INPUT/broadcast.mp4`
  * `INPUT/tacticam.mp4`
* Ensure your YOLO model (`best.pt`) is trained to detect players (and optionally ball)
* Works on CPU or GPU (CUDA auto-detected)

---

## ğŸ”§ Optional Improvements

* Iterative homography refinement via RANSAC (WIP)
* Match heatmap or interactive UI with Streamlit
* Add ground-truth comparison metrics (accuracy, recall, etc.)

---

## ğŸ†˜ Need Help?

* Use Python 3.8 to 3.11
* Run inside a clean virtual environment
* Check YOLO model format and video paths

---

```

Let me know if you'd like this written to a `README.md` file or with GitHub badges, links, or example images.
```
